import numpy as np
import torch
from utils import *
import argparse
import os
import pandas as pd
from datetime import datetime
import random 

parser = argparse.ArgumentParser(description='CH-HNN learning rules.')

## Initial setup for dataset and scenarios
parser.add_argument('--scenario', type = str, default = 'task-incre', metavar = 'Scenario', help='task- or class- incremental learning')
parser.add_argument('--dataset', type = str, default = 'cifar100', metavar = 'Dataset', help='dataset')
parser.add_argument('--task-num', type = int, default = 5, metavar = 'out', help='output size')
parser.add_argument('--class-per-task', type = int, default = 5, metavar = 'out', help='output size')
parser.add_argument('--task-sequence', nargs = '+', type = int, default = [5,], metavar = 'out', help='output size')
parser.add_argument('--device', type = int, default = 1)

## Helper neural networks setup (ANN)
parser.add_argument('--ann-channels', type = int, default = 256)
parser.add_argument('--sort-gate-train', type = str, default = "sort", help='whether sort target to generate gate when genaralize mean gate in training')
parser.add_argument('--sort-gate-test', type = str, default = "sort", help='whether sort target to generate gate when genaralize mean gate in testing')
parser.add_argument('--gate-type', type = str, default = 'pre-generated')

## Main neural networks setup (SNN)
parser.add_argument('--net', type = str, default = 'snn', metavar = 'NT', help='Type of net')
parser.add_argument('--neuron-model', type = str, default = 'LIF')
parser.add_argument('--tau', type = float, default = 0.25, metavar = 'SNN tau')
parser.add_argument('--wins', type = int, default = 10)

parser.add_argument('--norm', type = str, default = '', metavar = 'Nrm', help='Normalization procedure')
parser.add_argument('--track-BN-stat', default = False, action = 'store_true', help='whether track BN state in learning')
parser.add_argument('--reset-BN', default = False, action = 'store_true', help='whether reset the BN state when learning new tasks')

parser.add_argument('--loss', type = str, default = 'ce', help='Loss choice, ce:cross_entropy, log_sigmoid:log(sigmoid)')
parser.add_argument('--in-size', type = int, default = 768, metavar = 'in', help='input size')
parser.add_argument('--batch-size', type = int, default = 128, metavar = 'Batch', help='Batch size')
parser.add_argument('--hidden-layers', nargs = '+', type = int, default = [64,64], metavar = 'HL', help='size of the hidden layers')
parser.add_argument('--out-size', type = int, default = 100, metavar = 'out', help='output size')
parser.add_argument('--out-off', type = int, default = 4, metavar = 'OUT off', help='when the out size of head is 1, add out_off extra labels to help model training')

## Continual learning rules
parser.add_argument('--load-bn', default = False, action = 'store_true', help='whether load BN state of other tasks')
parser.add_argument('--type-of-head', type = str, default = 'mask', metavar = 'NT', help='Type of net')

## EWC or SI rules.
parser.add_argument('--ewc', default = False, action = 'store_true', help='whether use EWC loss')
parser.add_argument('--ewc-lambda', type = float, default = 0.0, metavar = 'Lbd', help='EWC coefficient')
parser.add_argument('--rnd-consolidation', default = False, action = 'store_true', help='use shuffled Elastic Weight Consolidation')
parser.add_argument('--si-lambda', type = float, default = 0.0, metavar = 'Lbd', help='SI coefficient')
parser.add_argument('--si', default = False, action = 'store_true', help='whether use SI loss')

## Metaplasticity rules.
parser.add_argument('--ann-meta', default = False, action = 'store_true', help='whether use meta modulation signals generated by ANN')
parser.add_argument('--meta', type = float, default = 0, metavar = 'single meta value')
parser.add_argument('--ms-path', type = str, default = '', metavar = 'Modulation Signal path')
parser.add_argument('--m-th', type = float, nargs = '+', default = [0], metavar = 'one-hot meta value')
parser.add_argument('--meta-ac', type = str, default = 'exp', metavar = 'Activation function of meta value', help='ac meta func')

## Gate rules.
parser.add_argument('--gate', default = False, action = 'store_true', help='whether use the gate mechanisms')
parser.add_argument('--gate-num', type = int, default = 0)

## XDG rules.
parser.add_argument('--xdg', default = False, action = 'store_true', help='whether use the gate mechanisms')
parser.add_argument('--gate-prob', type = float, default = 0.5, help='random gate probability of xdg methods, default:0.5')

## Training procedure setup
parser.add_argument('--optim', type = str, default = 'SGD', metavar = 'NT', help='Type of optimizer')
parser.add_argument('--save', type = bool, default = True, metavar = 'S', help='Saving the results')
parser.add_argument('--seed', type = int, default = 222, metavar = 'seed', help='seed for reproductibility')
parser.add_argument('--decay', type = float, default = 1e-7, metavar = 'dc', help='Weight decay')
parser.add_argument('--epochs-per-task', type = int, default = 5, metavar = 'out', help='output size')
parser.add_argument('--lr', type = float, default = 0.1, metavar = 'learning rate')
parser.add_argument('--gamma', type = float, default = 1.0, metavar = 'G', help='dividing factor for lr decay')

## save or showing setup
parser.add_argument('--save-model', type = bool, default = True, metavar = 'S', help='Saving the trained model')
parser.add_argument('--bin-path', default = False, action = 'store_true', help='path integral on binary weight, else perform path integral on hidden weight')

args = parser.parse_args()
os.environ['CUDA_VISIBLE_DEVICES'] = str(args.device)
args.task_offset = [0]
if args.dataset=='pMNIST':
    args.task_sequence = args.task_sequence*args.task_num
    print('task_sequence:', args.task_sequence)
else:
    args.task_sequence+=[args.class_per_task for i in range(args.task_num)]
    print('task_sequence:', args.task_sequence)
    if args.dataset=='cifar100':    
        assert np.sum(args.task_sequence)==100, "Tasks do not contain all the data in the CIFAR100!"
        args.out_size=100
    elif args.dataset=='tinyimage':
        assert np.sum(args.task_sequence)==200, "Tasks do not contain all the data in the CIFAR100!"
        args.out_size=200
if args.scenario=='class-incre' and args.type_of_head=='head':
    args.out_size = args.task_sequence[0]
elif args.scenario=='class-incre' and args.type_of_head=='mask':
    args.out_size = args.out_size
elif args.scenario=='task-incre':
    args.out_size = args.class_per_task
elif args.scenario=='upperbound-class':
    args.out_size = args.out_size
elif args.scenario=='upperbound-task':
    args.out_size = args.class_per_task

archi = [args.in_size] + args.hidden_layers + [args.out_size]
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
ewc_lambda = args.ewc_lambda
si_lambda = args.si_lambda

#process m_th as gate;
if len(args.m_th)==1:
    args.m_th = [args.meta]*(10-args.gate_num)+[np.inf]*args.gate_num
else:
    args.m_th = args.m_th+[np.inf]*args.gate_num
args.m_th=np.array(args.m_th).astype(np.float32)

## set the random seed
if args.seed is not None:
    torch.manual_seed(args.seed)
    torch.cuda.manual_seed_all(args.seed)
    torch.cuda.manual_seed(args.seed)
    np.random.seed(args.seed)
    random.seed(args.seed)
    torch.backends.cudnn.deterministic=True

date = datetime.now().strftime('%Y-%m-%d')
time = datetime.now().strftime('%H-%M-%S')
path = '/data1/sqq/Results/{}/'.format(args.dataset)+date+'/'+time+'_gpu'+str(args.device)

if not(os.path.exists(path)):
    os.makedirs(path)

if args.net=='snn':
    if args.dataset=='pMNIST':
        model = SNN_pMNIST(archi, wins=args.wins, tau=args.tau, norm=args.norm, neu_model=args.neuron_model, tracking_stat=args.track_BN_stat).to(device)
    else:
        model = SNN(archi, wins=args.wins, tau=args.tau, norm=args.norm, neu_model=args.neuron_model, track_BN_state=args.track_BN_stat, out_off = args.out_off).to(device)
elif args.net=='ann':
    model = ANN(archi, norm=args.norm, track_BN_state=args.track_BN_stat)
model = model.to(device)

train_loader_list, test_loader_list, dset_train_list = create_dataset(args)
print('task_offset:', args.task_offset)
print('train_loader_list:', len(train_loader_list))
lrs = [args.lr*(args.gamma**(-i)) for i in range(len(train_loader_list))]

# Data collect initialisation

data = data_initial(args, test_loader_list)
name = '_'+data['net']
bn_states = []
if args.gate and (args.gate_type=='generate-train-test' or args.gate_type=='generate-test'):
    ann_trained = aux_net(inputs_size=args.in_size, hidden_size_list=args.hidden_layers, 
                      channels=args.ann_channels, n_value=10).to(device)
    state_dict = torch.load(args.ms_path+'/models/gate_net.pth')
    new_state_dict = {}
    for key, value in state_dict.items():
        if 'linear_2_1' in key:
            new_key = key.replace('linear_2_1', 'linear_2')
        else:
            new_key = key
        new_state_dict[new_key]=value
    ann_trained.load_state_dict(new_state_dict)
    gate=[]
elif args.gate and args.gate_type=='pre-generated':
    gate = gate_load(args, device, args.gate_prob)
    ann_trained=None
else:
    ann_trained=None
    gate=[]

createHyperparametersFile(path, args)

# EWC and SI parameters initialization
previous_tasks_parameters = {}
previous_tasks_fisher = {}
if args.ewc:
    for n, p in model.named_parameters():
        if n.find('bn') == -1: #we dont store bn parameters as we allow task dependent bn
            n = n.replace('.', '__')
            previous_tasks_fisher[n] = []
            previous_tasks_parameters[n] = [] 
elif args.si:
    W = {}
    p_prev = {}
    p_old = {}
    omega = {}
    for n, p in model.named_parameters():
        if p.requires_grad:
            n = n.replace('.', '__')
            W[n] = p.data.clone().zero_()
            omega[n] = p.data.clone().zero_()
            if args.net=='snn':
                p_prev[n] = p.data.clone()  # or sign
                if args.bin_path:
                    p_old[n] = p.data.sign().clone()
                else:
                    p_old[n] = p.data.clone()
            elif args.net=='ann':
                p_prev[n] = p.data.clone()
                p_old[n] = p.data.clone()

weight_plot=0
for task_idx, task in enumerate(train_loader_list):
    print('task_idx:', task_idx)
    # generate parameters for training
    if args.scenario=='class-incre' and args.type_of_head=='head':
        model.add_head(args.task_sequence[task_idx])
        model.to(device)
        weight_plot+=1
    meta = meta_generate(args, model, device)
    gate_train = get_gate(args, gate, task_idx, device, 'train')
    optimizer = optimizer_utils(args, model, lrs, meta, task_idx)
    for epoch in range(1, args.epochs_per_task+1):
        if args.ewc:
            train(model, ann_trained, task, task_idx, optimizer, device, args, gate_train, prev_cons=previous_tasks_fisher, 
                    prev_params=previous_tasks_parameters)
        elif args.si:
            train(model, ann_trained, task, task_idx, optimizer, device, args, gate_train, prev_cons=omega, path_integ=W, prev_params=(p_prev, p_old) ) 
        else:
            train(model, ann_trained, task, task_idx, optimizer, device, args, gate_train)
        data['task_order'].append(task_idx+1)
        data['epoch'].append(epoch)
        data['lr'].append(optimizer.param_groups[0]['lr'])
        data['ewc'].append(ewc_lambda)
        data['SI'].append(si_lambda)
        train_accuracy_tag, train_loss = test(model, ann_trained, task, task_idx, device, args, gate=gate_train, verbose=True)
        print('Task idx:', task_idx, 'Epoch:', epoch, 'Train Accuracy Tag:', 
              train_accuracy_tag, 'Train Loss:', train_loss)
        
        # data['acc_tr_taw'].append(train_accuracy_taw)
        data['acc_tr_tag'].append(train_accuracy_tag)
        data['loss_tr'].append(train_loss)

        if 'n' in args.norm:
            current_bn_state = model.save_bn_states()#every task use the same BN parameters(including mean and variance)
        print("Evaluating...")
        for other_task_idx, other_task in enumerate(test_loader_list):
            if epoch==args.epochs_per_task:
                gate_test = get_gate(args, gate, other_task_idx, device, 'test')
                if 'n' in args.norm and args.load_bn:
                    model.load_bn_states(current_bn_state) if other_task_idx>=task_idx else model.load_bn_states(bn_states[other_task_idx])
                elif 'n' in args.norm:
                    model.load_bn_states(current_bn_state)

                if 'upperbound' in args.scenario:
                    test_accuracy_tag, test_loss = test(model, ann_trained, other_task, other_task_idx, device, args, gate=gate_test, verbose=(other_task_idx==task_idx))
                elif args.scenario=='task-incre' and other_task_idx<=task_idx:
                    test_accuracy_tag, test_loss = test(model, ann_trained, other_task, other_task_idx, device, args, gate=gate_test, verbose=(other_task_idx==task_idx))
                elif args.scenario=='class-incre':
                    test_accuracy_tag, test_loss = test(model, ann_trained, other_task, other_task_idx, device, args, gate=gate_test, verbose=(other_task_idx==task_idx*args.class_per_task))
                else:
                    test_accuracy_tag, test_loss = 0, 0
            else:
                test_accuracy_tag, test_loss = 0, 0

                # data['acc_test_taw_tsk_'+str(other_task_idx+1)].append(test_accuracy_taw)

            data['acc_test_tsk_'+str(other_task_idx+1)].append(test_accuracy_tag)
            data['loss_test_tsk_'+str(other_task_idx+1)].append(test_loss)
    if 'n' in args.norm:
        bn_states.append(current_bn_state)
        if args.track_BN_stat and args.reset_BN:
            model.reset_bn_states()
    # EWC or SI implementation
    if args.ewc:
        fisher = estimate_fisher(model, dset_train_list[task_idx], args, device, task_idx, num=5000, empirical=True)
        for n, p in model.named_parameters():
            if n.find('bn') == -1:# not batchnorm
                n = n.replace('.', '__')
            
                # random consolidation
                if args.rnd_consolidation:
                    idx = torch.randperm(fisher[n].nelement())
                    previous_tasks_fisher[n].append(fisher[n].view(-1)[idx].view(fisher[n].size()))
            
                # EWC consolidation, comment when using random consolidation
                previous_tasks_fisher[n].append(fisher[n])
                previous_tasks_parameters[n].append(p.detach().clone())
    elif args.si:
        omega = update_omega(model, omega, p_prev, W)
        for n, p in model.named_parameters():
            if n.find('bn') == -1: # not batchnorm
                n = n.replace('.','__')
                if args.net=='bnn':
                    p_prev[n] = p.org.detach().clone()  # or sign
                else:
                    p_prev[n] = p.detach().clone()


time = datetime.now().strftime('%H-%M-%S')
if args.save_model:
    torch.save(model.state_dict(), path+'/'+time+'_SNN_state_dict.pt')
    torch.save(model, path+'/'+time+'_SNN.pt')
    if args.norm=='bn' and args.track_BN_stat:
        torch.save(bn_states, path+'/'+time+'_BatchNorm_state.pth')
df_data = pd.DataFrame(data)
# plot_results(df_data, path, length=len(test_loader_list), args=args, save=args.save)

if args.save:
    df_data.to_csv(path +'/'+time+name+'.csv', index = False)
